{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "796ccac0-fd29-42da-850f-fdeca7a1082d",
    "_uuid": "0962cd02-9f6a-4950-beaf-7e4a63c1d48e"
   },
   "outputs": [],
   "source": [
    "get_ipython().system('pip install scikit-hts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ace6126-d32c-485b-baa2-785cd6778eb3",
    "_uuid": "5f80b8bd-6b3a-4d48-9ec9-73f5e4464b1d"
   },
   "outputs": [],
   "source": [
    "get_ipython().system('pip install pmdarima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f028d4f9-6ebe-4d12-a01a-e8c0414ee289",
    "_uuid": "d3b64371-40ce-4912-a118-51e5d8a3752e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cb57eabd-b4d2-4e4c-99b1-fe8c5849d5fa",
    "_uuid": "0c28d44a-388a-4c48-97af-7ac98c3c9f4a"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return{\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty \n",
    "    }\n",
    "\n",
    "prophet_defaults = get_default_args(Prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eaafde9b-f78c-464c-9ce5-fbe63a7e6acd",
    "_uuid": "750f59f3-0ac8-454f-9be6-c55db9af9afb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#df = pd.read_csv(\"/kaggle/input/sales-df/df.csv\")\n",
    "df = pd.read_csv(\"data/df.csv\")\n",
    "#with open(\"/kaggle/input/hier-json/hierarchy.json\", \"r\") as f:\n",
    "with open(\"data/hierarchy.json\", \"r\") as f:\n",
    "    hierarchy = json.load(f)  \n",
    "if \"Date\" in df.columns.tolist():\n",
    "    df.index = pd.to_datetime(df[\"Date\"])\n",
    "    del df[\"Date\"]\n",
    "df = df.asfreq(\"D\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1ad907a-8f65-4c35-884a-87f217dd2932",
    "_uuid": "b76be974-d8e7-4250-9e18-6bbc6cf5736a"
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/hier-json/hierarchy.json\", \"r\") as f:\n",
    "    hierarchy = json.load(f)\n",
    "df_path = \"/kaggle/input/df-add-lookup/df_add_lookup.csv\"\n",
    "df = pd.read_csv(df_path)\n",
    "shift_cols = list(filter(lambda x: x.startswith(\"Customers\") or x.startswith(\"Sales\"),df.columns.tolist()))\n",
    "for col in shift_cols:\n",
    "    df[col] = df[col].shift(1)\n",
    "#df = df.dropna()\n",
    "df = df.fillna(0)\n",
    "if \"Date\" in df.columns.tolist():\n",
    "    df.index = pd.to_datetime(df[\"Date\"])\n",
    "    del df[\"Date\"]\n",
    "df = df.asfreq(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dce35309-93b7-434b-825a-c0904b36093b",
    "_uuid": "d723f3f5-1fb4-41b1-98de-e0e691b2bdfc"
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0998f73d-21cd-4928-8a90-4d5c7b4ac297",
    "_uuid": "7f0c6181-136c-4eba-8a24-3a043de5f7a4"
   },
   "outputs": [],
   "source": [
    "#first_df, second_df = pd.read_csv(\"data/df.csv\"), pd.read_csv(\"df_add_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6367b5fd-851f-4749-a380-9b7f23a7dfbb",
    "_uuid": "edccfcf3-d78e-47b6-9b9b-a24101d8c4f0"
   },
   "outputs": [],
   "source": [
    "#first_df.shape, second_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19fe4012-d618-4b3b-bf2c-810638b250d5",
    "_uuid": "b273cab2-b202-44bf-986a-3ec698652fc8"
   },
   "outputs": [],
   "source": [
    "#(first_df.iloc[:, 1:] - second_df[first_df.iloc[:, 1:].columns.tolist()]).applymap(np.abs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef12a111-3d59-4c82-9493-c9dfcc3fc916",
    "_uuid": "fff0cc64-8155-49e7-bc34-d4575c428f0c"
   },
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "419682d5-13f7-4461-8522-07b6eea0e74d",
    "_uuid": "34abeb2f-44e3-49e8-b828-6c8991e7149f"
   },
   "outputs": [],
   "source": [
    "#from hts.hierarchy import HierarchyTree\n",
    "#ht = HierarchyTree.from_nodes(nodes = hierarchy, df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "afd03a0a-82c8-4336-a86c-f454b91f58df",
    "_uuid": "d811d7cf-7e07-4b83-b76b-4a15decd66a0"
   },
   "outputs": [],
   "source": [
    "#type(ht.children[0]), type(ht)\n",
    "#ht.key, ht.children[0].key\n",
    "#ht.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d36cfce9-8477-4ce7-89f5-bea4485a3889",
    "_uuid": "b77d3776-cfcc-478f-9113-8940bc94950e"
   },
   "outputs": [],
   "source": [
    "#list(hierarchy.keys()) + list(hierarchy.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9c35c900-9791-4d08-965a-8fec3693b13d",
    "_uuid": "83bbb913-bcd9-4df7-ab5a-3a5bc08433a2"
   },
   "outputs": [],
   "source": [
    "head_set = set(filter(lambda x: \"_\" in x ,map(lambda col: col[:col.rfind(\"_\")] ,df.columns.tolist())))\n",
    "cate_set = set(filter(lambda x: \"_\" not in x ,map(lambda col: col[col.rfind(\"_\") + 1:] ,df.columns.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32f33e70-e41a-4f6e-ad5e-bedb635ccf11",
    "_uuid": "64c8f8f3-8d6f-43d8-b31b-9bb4a9ba1b82"
   },
   "outputs": [],
   "source": [
    "#head_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0a58a3a-93d9-4631-9b92-11fdd2351207",
    "_uuid": "445e0fd8-4769-4c57-8268-dc2bc16ae170"
   },
   "outputs": [],
   "source": [
    "#\"Promo2SinceWeek_mean_total\" in df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7d88c10-e130-409f-9cc4-3c1582a10721",
    "_uuid": "ec846afe-ab20-41b1-a5c8-f2450fc9a5c4"
   },
   "outputs": [],
   "source": [
    "from hts.model import FBProphetModel\n",
    "from hts.model.p import *\n",
    "class FBProphetModel_Add_Version(FBProphetModel):\n",
    "    def create_model(self, *args, **kwargs):\n",
    "        self.lookup_df = None\n",
    "        has_children = bool(self.node.children)\n",
    "        is_children = not has_children\n",
    "        print(\"will create {} model {}\".format(\"leaf\" if is_children else \"branch\", self.node) + \"-\" * 10)\n",
    "        if \"ext_args\" not in kwargs:\n",
    "            return super(FBProphetModel_Add_Version, self).create_model(*args, **kwargs)\n",
    "        base_kwargs = dict()\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in [\"ext_args\", \"add_reg\"]:\n",
    "                base_kwargs[k] = v\n",
    "        base_model = super(FBProphetModel_Add_Version, self).create_model(*args, **base_kwargs)\n",
    "        #base_model = super().create_model(*args, **kwargs)\n",
    "        ext_args_dict = kwargs[\"ext_args\"]\n",
    "        if \"seasonality\" in ext_args_dict:\n",
    "            assert type(ext_args_dict[\"seasonality\"]) == type([])\n",
    "            for seasonality_dict in ext_args_dict[\"seasonality\"]:\n",
    "                name, period, fourier_order, prior_scale = seasonality_dict[\"name\"], seasonality_dict[\"period\"], seasonality_dict[\"fourier_order\"], seasonality_dict[\"prior_scale\"]\n",
    "                base_model.add_seasonality(name = name, period = period, fourier_order = fourier_order, prior_scale =prior_scale)\n",
    "                print(\"seasonality {} {} {} {} added\".format(name, period, fourier_order, prior_scale))\n",
    "        if \"add_reg\" in kwargs and \"data\" in kwargs[\"add_reg\"] and kwargs[\"add_reg\"][\"data\"] is not None:\n",
    "            assert \"reg\" in ext_args_dict\n",
    "            print(\"reg\")\n",
    "            reg_prior_dict = ext_args_dict[\"reg\"]\n",
    "            self.lookup_df = kwargs[\"add_reg\"][\"data\"]\n",
    "            cate = self.node.key\n",
    "            assert cate in cate_set\n",
    "            for col in map(lambda x: \"{}_{}\".format(x, cate), head_set):\n",
    "                if \"_mean_\" in col:\n",
    "                    prior_scale = reg_prior_dict[\"{}_prior\".format(col)]\n",
    "                    base_model.add_regressor(col, prior_scale = prior_scale)\n",
    "                    #print(\"reg {} added\".format(col))\n",
    "        return base_model\n",
    "    def fit(self, **fit_args) -> 'TimeSeriesModel':\n",
    "        df = self._reformat(self.node.item)\n",
    "\n",
    "        if self.lookup_df is not None:\n",
    "            lookup_df = self.lookup_df\n",
    "            before_shape = df.shape\n",
    "            if \"Date\" not in lookup_df.columns.tolist():\n",
    "                lookup_df = lookup_df.reset_index()\n",
    "            ds_col = list(map(lambda tt2: tt2[0] ,filter(lambda t2: t2[-1] ,lookup_df.dtypes.map(str).map(lambda x: x == \"datetime64[ns]\").to_dict().items())))[0]\n",
    "            lookup_df = lookup_df.rename(columns = {ds_col: \"ds\"})\n",
    "            df = pd.merge(df, lookup_df, on = \"ds\", how = \"inner\")\n",
    "            print(\"fit merge shape {} before shape {}\".format(df.shape, before_shape))\n",
    "            \n",
    "        with suppress_stdout_stderr():\n",
    "            self.model = self.model.fit(df)\n",
    "            self.model.stan_backend = None\n",
    "        return self\n",
    "\n",
    "    def predict(self,\n",
    "                node: HierarchyTree,\n",
    "                freq: str = 'D',\n",
    "                steps_ahead: int = 1):\n",
    "\n",
    "        df = self._reformat(node.item)\n",
    "        future = self.model.make_future_dataframe(periods=steps_ahead,\n",
    "                                                  freq=freq,\n",
    "                                                  include_history=True)\n",
    "        if self.cap:\n",
    "            future['cap'] = self.cap\n",
    "        if self.floor:\n",
    "            future['floor'] = self.floor\n",
    "        \n",
    "        if self.lookup_df is not None:\n",
    "            lookup_df = self.lookup_df\n",
    "            before_shape = future.shape\n",
    "            if \"Date\" not in lookup_df.columns.tolist():\n",
    "                lookup_df = lookup_df.reset_index()\n",
    "            ds_col = list(map(lambda tt2: tt2[0] ,filter(lambda t2: t2[-1] ,lookup_df.dtypes.map(str).map(lambda x: x == \"datetime64[ns]\").to_dict().items())))[0]\n",
    "            lookup_df = lookup_df.rename(columns = {ds_col: \"ds\"})\n",
    "            future = pd.merge(future, lookup_df, on = \"ds\", how = \"inner\")\n",
    "            print(\"pred merge shape {} before shape {}\".format(future.shape, before_shape))\n",
    "        \n",
    "        self.forecast = self.model.predict(future)\n",
    "        merged = pandas.merge(df, self.forecast, on='ds')\n",
    "        self.residual = (merged['yhat'] - merged['y']).values\n",
    "        self.mse = numpy.mean(numpy.array(self.residual) ** 2)\n",
    "        if self.cap is not None:\n",
    "            self.forecast.yhat = numpy.exp(self.forecast.yhat)\n",
    "        if self.transform:\n",
    "            self.forecast.yhat = self.transformer.inverse_transform(self.forecast.yhat)\n",
    "            self.forecast.trend = self.transformer.inverse_transform(self.forecast.trend)\n",
    "            for component in [\"seasonal\", \"daily\", \"weekly\", \"yearly\", \"holidays\"]:\n",
    "                if component in self.forecast.columns.tolist():\n",
    "                    inv_transf = self.transformer.inverse_transform(getattr(self.forecast, component))\n",
    "                    setattr(self.forecast, component, inv_transf)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "baf790b1-a6af-448e-a6bd-1ee61597e2d9",
    "_uuid": "c58a8c9c-250e-4bf7-aae0-e7a60c2b80bd"
   },
   "outputs": [],
   "source": [
    "from hts.revision import *\n",
    "class RevisionMethod_BU(RevisionMethod):\n",
    "    def _y_hat_matrix(self, forecasts):\n",
    "        print(\"forecasts first key: {}\".format(list(forecasts.keys())[0]))\n",
    "        n_cols = len(list(forecasts.keys())) + 1\n",
    "        keys = range(n_cols - self.sum_mat.shape[1] - 1, n_cols - 1)\n",
    "        #named_keys = list(map(lambda k_idx: list(forecasts.keys())[k_idx], keys))\n",
    "        named_keys = list(set(forecasts.keys()).difference(set([\"total\"])))\n",
    "        return y_hat_matrix(forecasts, keys=named_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6bdc5c4d-571c-4e0b-a3c8-f8d60209eb55",
    "_uuid": "4954bf85-a86e-4a59-ad9e-c86045fa4929"
   },
   "outputs": [],
   "source": [
    "from hts.core.regressor import *\n",
    "class HTSRegressor_Add_Verison(HTSRegressor):\n",
    "    def _init_revision(self):\n",
    "        self.revision_method = RevisionMethod_BU(sum_mat=self.sum_mat, transformer=self.transform, name=self.method)\n",
    "    def _set_model_instance(self):\n",
    "        from hts import model as hts_models\n",
    "        from copy import deepcopy\n",
    "        model_mapping = deepcopy(hts_models.MODEL_MAPPING)\n",
    "        model_mapping[\"prophet\"] = FBProphetModel_Add_Version\n",
    "        try:\n",
    "            self.model_instance = model_mapping[self.model]\n",
    "        except KeyError:\n",
    "            raise InvalidArgumentException(f'Model {self.model} not valid. Pick one of: {\" \".join(Model.names())}')\n",
    "            \n",
    "    def _revise(self, steps_ahead=1):\n",
    "        logger.info(f'Reconciling forecasts using {self.revision_method}')\n",
    "        revised = self.revision_method.revise(\n",
    "            forecasts=self.hts_result.forecasts,\n",
    "            mse=self.hts_result.errors,\n",
    "            nodes=self.nodes\n",
    "        )\n",
    "\n",
    "        revised_columns = list(make_iterable(self.nodes))\n",
    "        revised_index = self._get_predict_index(steps_ahead=steps_ahead)\n",
    "        return pandas.DataFrame(revised,\n",
    "                                index=revised_index,\n",
    "                                columns=revised_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "edce5b19-502a-4f2d-a7f8-9d6b6342457a",
    "_uuid": "3941a96d-f9da-4007-bfaf-ac43cbcc3abc"
   },
   "outputs": [],
   "source": [
    "def object_func(kw):\n",
    "    from copy import deepcopy\n",
    "    from time import time\n",
    "    from fbprophet.diagnostics import generate_cutoffs, performance_metrics\n",
    "    start_time = time()\n",
    "    assert type(kw) == type(dict())\n",
    "    \n",
    "    n_changepoints = int(kw[\"n_changepoints\"])\n",
    "    changepoint_range = kw[\"changepoint_range\"]\n",
    "    seasonality_prior_scale = kw[\"seasonality_prior_scale\"]\n",
    "    holidays_prior_scale = kw[\"holidays_prior_scale\"]\n",
    "    changepoint_prior_scale = kw[\"changepoint_prior_scale\"]\n",
    "    \n",
    "    yearly_fourier_order = int(kw[\"yearly_fourier_order\"])\n",
    "    weekly_fourier_order = int(kw[\"weekly_fourier_order\"])\n",
    "    monthly_fourier_order = int(kw[\"monthly_fourier_order\"])\n",
    "\n",
    "    yearly_prior = kw[\"yearly_prior\"]\n",
    "    weekly_prior = kw[\"weekly_prior\"]\n",
    "    monthly_prior = kw[\"monthly_prior\"]\n",
    "\n",
    "    kwargs = dict()\n",
    "    kwargs[\"yearly_seasonality\"] = False\n",
    "    kwargs[\"daily_seasonality\"] = False\n",
    "    kwargs[\"weekly_seasonality\"] = False\n",
    "    kwargs[\"ext_args\"] = {\n",
    "    \"seasonality\": [],\n",
    "    \"reg\": {}\n",
    "}\n",
    "\n",
    "    kwargs[\"ext_args\"][\"seasonality\"].append(\n",
    "    {\n",
    "        \"name\": \"yearly\",\n",
    "        \"period\": 365.25,\n",
    "        \"fourier_order\": yearly_fourier_order,\n",
    "        \"prior_scale\": yearly_prior\n",
    "    }\n",
    ")\n",
    "    kwargs[\"ext_args\"][\"seasonality\"].append(\n",
    "    {\n",
    "        \"name\": \"weekly\",\n",
    "        \"period\": 7,\n",
    "        \"fourier_order\": weekly_fourier_order,\n",
    "        \"prior_scale\": weekly_prior\n",
    "    }\n",
    ")\n",
    "    kwargs[\"ext_args\"][\"seasonality\"].append(\n",
    "    {\n",
    "        \"name\": \"monthly\",\n",
    "        \"period\": 30.5,\n",
    "        \"fourier_order\": monthly_fourier_order,\n",
    "        \"prior_scale\": monthly_prior\n",
    "    }\n",
    ")\n",
    "    \n",
    "    kwargs[\"n_changepoints\"] = n_changepoints\n",
    "    kwargs[\"seasonality_prior_scale\"] = seasonality_prior_scale\n",
    "    kwargs[\"holidays_prior_scale\"] = holidays_prior_scale\n",
    "    kwargs[\"changepoint_prior_scale\"] = changepoint_prior_scale\n",
    "    \n",
    "    kwargs[\"changepoint_range\"] = changepoint_range\n",
    "    kwargs[\"add_reg\"] = {\n",
    "        \"data\": None, \n",
    "    }\n",
    "    \n",
    "    horizon = \"7 days\"\n",
    "    period = \"7 days\"\n",
    "    initial = \"870 days\"\n",
    "    \n",
    "    horizon, period, initial = map(pd.Timedelta, [horizon, period, initial])\n",
    "    #df = pd.read_csv(\"/kaggle/input/sales-df/df.csv\")\n",
    "    #df = pd.read_csv(\"data/df.csv\")\n",
    "    df = pd.read_csv(df_path)\n",
    "    shift_cols = list(filter(lambda x: x.startswith(\"Customers\") or x.startswith(\"Sales\"),df.columns.tolist()))\n",
    "    #shift_cols = []\n",
    "    for col in shift_cols:\n",
    "        df[col] = df[col].shift(1).copy()\n",
    "    #df = df.dropna()\n",
    "    df = df.fillna(0)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    \n",
    "    kwargs[\"add_reg\"][\"data\"] = df\n",
    "    \n",
    "    other_cols = list(filter(lambda col: col not in [\"Date\"] + list(hierarchy.keys()) + list(hierarchy.values())[0] ,df.columns.tolist()))\n",
    "    for col in other_cols:\n",
    "        #exec(\"reg_{}_prior=kw['{}_prior']\".format(col, col))\n",
    "        #kwargs[\"reg\"][\"{}_prior\".format(col)] = kw[\"{}_prior\".format(col)]\n",
    "        kwargs[\"ext_args\"][\"reg\"][\"{}_prior\".format(col)] = kw[\"{}_prior\".format(col)]\n",
    "    #df = df[[\"Date\"] + list(hierarchy.keys()) + list(hierarchy.values())[0]].copy()\n",
    "\n",
    "    cutoffs = generate_cutoffs(df = df.reset_index().rename(columns ={\"Date\": \"ds\", \"total\": \"y\"})[[\"ds\", \"y\"]],horizon=horizon, period=period, initial=initial)\n",
    "\n",
    "    cutoffs_list = list(cutoffs)\n",
    "\n",
    "    def cutoff_forecast_gen(input_df, cutoffs_list):\n",
    "        if \"Date\" not in input_df.columns.tolist() and \"ds\" not in input_df.columns.tolist():\n",
    "            input_df = input_df.reset_index() \n",
    "            ds_col = \"Date\"\n",
    "        else:\n",
    "            ds_col = \"Date\" if \"Date\" in input_df.columns.tolist() else \"ds\"\n",
    "        assert ds_col in input_df.columns.tolist()\n",
    "        input_df[ds_col] = pd.to_datetime(input_df[ds_col])\n",
    "        for i in range(len(sorted(cutoffs_list)[:-1])):\n",
    "            time = sorted(cutoffs_list)[i]\n",
    "            df_for_fit = input_df[input_df[ds_col] <= time].copy()\n",
    "            pred_length = (sorted(cutoffs_list)[i + 1] - time).days\n",
    "            df_for_fit.index = df_for_fit[ds_col]\n",
    "            del df_for_fit[ds_col]\n",
    "            yield (df_for_fit, pred_length)\n",
    "\n",
    "    cutoffs_gen = cutoff_forecast_gen(df, cutoffs_list=cutoffs_list)\n",
    "\n",
    "    cutoffs_gen_list = list(cutoffs_gen)\n",
    "\n",
    "    def single_fit_predict(total_df ,clf_for_cp, df_for_fit, hierarchy, pred_length, measure_cols = \"total\", return_errors = True):\n",
    "        assert type(measure_cols) in [type([]), type(\"\")]\n",
    "        measure_cols_list = measure_cols if type(measure_cols) == type([]) else [measure_cols]\n",
    "        from copy import deepcopy\n",
    "        clf = deepcopy(clf_for_cp)\n",
    "        df_for_fit = df_for_fit.asfreq(\"D\")\n",
    "        model = clf.fit(df_for_fit, hierarchy)\n",
    "        preds = model.predict(steps_ahead=pred_length)\n",
    "        if \"Date\" not in total_df.columns.tolist():\n",
    "            total_df = total_df.reset_index()\n",
    "        if \"Date\" not in preds.columns.tolist():\n",
    "            preds = preds.reset_index()\n",
    "        \n",
    "        total_ds_col = list(map(lambda tt2: tt2[0] ,filter(lambda t2: t2[-1] ,total_df.dtypes.map(str).map(lambda x: x == \"datetime64[ns]\").to_dict().items())))[0]\n",
    "        preds_ds_col = list(map(lambda tt2: tt2[0] ,filter(lambda t2: t2[-1] ,preds.dtypes.map(str).map(lambda x: x == \"datetime64[ns]\").to_dict().items())))[0]\n",
    "        preds_lookup_part = preds.sort_values(by = preds_ds_col, ascending = True).tail(pred_length).copy()\n",
    "        total_lookup_part = total_df[total_df[total_ds_col].isin(preds_lookup_part[preds_ds_col])]\n",
    "        print(\"total_part {} preds_part {}\".format(total_lookup_part.shape, preds_lookup_part.shape))\n",
    "        assert len(set(measure_cols_list).intersection(set(total_lookup_part.columns.tolist()))) == len(measure_cols_list)\n",
    "        assert len(set(measure_cols_list).intersection(set(preds_lookup_part.columns.tolist()))) == len(measure_cols_list)\n",
    "        preds_lookup_part = preds_lookup_part[preds_lookup_part[preds_ds_col].isin(total_lookup_part[total_ds_col])]\n",
    "        total_lookup_part = total_lookup_part.sort_values(by = total_ds_col, ascending = True)\n",
    "        preds_lookup_part = preds_lookup_part.sort_values(by = preds_ds_col, ascending = True)\n",
    "        if not return_errors:\n",
    "            return (total_lookup_part[measure_cols_list], preds_lookup_part[measure_cols_list])\n",
    "        \n",
    "        measure_cols_errors = (total_lookup_part[measure_cols_list] - preds_lookup_part[measure_cols_list]).applymap(abs).mean(axis = 0)\n",
    "        mean_errors = measure_cols_errors.mean()\n",
    "        return (measure_cols_errors, mean_errors)\n",
    "    \n",
    "    clf = HTSRegressor_Add_Verison(model='prophet', revision_method='BU', n_jobs=4, low_memory=False, **kwargs)\n",
    "    \n",
    "    def construct_df_cv_format(ori_df, pred_df, max_date, add_one = True):\n",
    "        assert ori_df.columns.tolist() == pred_df.columns.tolist() and ori_df.shape == pred_df.shape\n",
    "        ori_series_list = []\n",
    "        pred_series_list = []\n",
    "        cols = ori_df.columns.tolist()\n",
    "        for col in cols:\n",
    "            ori_series_list.append(ori_df[col])\n",
    "            pred_series_list.append(pred_df[col])\n",
    "        \n",
    "        def bind_single_series(ori_s, pred_s):\n",
    "            bind_df = pd.concat(list(map(pd.Series,[ori_s.values.tolist(), pred_s.values.tolist()])), axis = 1)\n",
    "            assert bind_df.shape[0] == len(ori_s)\n",
    "            bind_df.columns = [\"y\", \"yhat\"]\n",
    "            if add_one:\n",
    "                bind_df[\"y\"] += 1\n",
    "                bind_df[\"yhat\"] += 1\n",
    "            bind_df[\"ds\"] = pd.date_range(max_date - pd.Timedelta(\"{}d\".format(len(ori_s) - 1)), max_date)\n",
    "            bind_df[\"cutoff\"] = max_date - pd.Timedelta(\"{}d\".format(len(ori_s)))\n",
    "            return bind_df\n",
    "\n",
    "        before_performance_list = list(map(lambda idx: ( ori_series_list[idx].name,bind_single_series(ori_series_list[idx], pred_series_list[idx])), range(len(ori_series_list))))\n",
    "        different_part_dict = dict(map(lambda t2: (t2[0] ,performance_metrics(t2[-1], [\"mape\"])), before_performance_list))\n",
    "        return dict(map(lambda t2: (t2[0], t2[1][\"mape\"].mean()) ,different_part_dict.items()))\n",
    "\n",
    "    use_mape = True\n",
    "    if use_mape:\n",
    "        cutoffs_run_conclusions = list(map(lambda df_and_step:  (pd.Series(df_and_step[0].index.tolist()).max() ,single_fit_predict(df[[\"Date\"] + list(hierarchy.keys()) + list(hierarchy.values())[0]] ,clf, df_and_step[0], hierarchy, df_and_step[1], [\"total\"], return_errors = False)), cutoffs_gen_list))\n",
    "        dict_list = list(map(lambda t2: construct_df_cv_format(t2[-1][0], t2[-1][1], t2[0]), cutoffs_run_conclusions))\n",
    "        from collections import defaultdict\n",
    "        req = defaultdict(list)\n",
    "        for dict_ in dict_list:\n",
    "            for k, v in dict_.items():\n",
    "                req[k].append(v)\n",
    "        req_cp = dict()\n",
    "        for k, v in req.items():\n",
    "            req_cp[k] = pd.Series(v).mean()\n",
    "        score = pd.Series(list(req_cp.values())).mean()\n",
    "        #return score\n",
    "    else:\n",
    "        cutoffs_run_conclusions = list(map(lambda df_and_step:  (pd.Series(df_and_step[0].index.tolist()).max() ,single_fit_predict(df ,clf, df_and_step[0], hierarchy, df_and_step[1], [\"total\"], return_errors = True)), cutoffs_gen_list))\n",
    "            \n",
    "        cutoffs_run_conclusions_df = pd.DataFrame(cutoffs_run_conclusions)\n",
    "        cutoffs_run_conclusions_df.columns = [\"Date\", \"error\"]\n",
    "        data_unzipped = pd.concat(list(map(pd.Series ,cutoffs_run_conclusions_df[\"error\"].map(lambda x: dict(x[0])).values.tolist())), axis = 1).T\n",
    "        cutoffs_run_conclusions_df[data_unzipped.columns.tolist()] = data_unzipped\n",
    "        cutoffs_run_conclusions_df = cutoffs_run_conclusions_df[[\"Date\"] + data_unzipped.columns.tolist()]\n",
    "        score = cutoffs_run_conclusions_df[\"total\"].mean()\n",
    "    \n",
    "    import pickle as pkl\n",
    "    import os\n",
    "    retain_size = 100\n",
    "    serlize_path = \"/kaggle/working/serlize_m_dict.pkl\"\n",
    "    if os.path.exists(serlize_path):\n",
    "        with open(serlize_path, \"rb\") as f:\n",
    "            ori_serlize_m_dict = pkl.load(f)\n",
    "        if len(ori_serlize_m_dict) > retain_size:\n",
    "            ori_serlize_m_dict = dict(list(sorted(ori_serlize_m_dict.items(), key = lambda t2: t2[0]))[:retain_size])\n",
    "    else:\n",
    "        ori_serlize_m_dict = {}\n",
    "    ori_serlize_m_dict = dict([(score, (kw, ))] + list(ori_serlize_m_dict.items()))\n",
    "    \n",
    "    ori_serlize_m_dict = dict(list(sorted(ori_serlize_m_dict.items(), key = lambda t2: t2[0]))[:retain_size])\n",
    "    print(\"list length : {}\".format(len(ori_serlize_m_dict)))\n",
    "    \n",
    "    with open(serlize_path, \"wb\") as f:\n",
    "        pkl.dump(ori_serlize_m_dict, f)\n",
    "    print(\"time consum : {}\".format(time() - start_time))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5051f858-c6ce-4ee8-9942-56cd36abf93f",
    "_uuid": "a94ae7d4-b438-4b99-a492-1799a0f9fed6"
   },
   "outputs": [],
   "source": [
    "req_keys = [\n",
    "         'n_changepoints',\n",
    "     'changepoint_range',\n",
    "     'seasonality_prior_scale',\n",
    "     'holidays_prior_scale',\n",
    "     'changepoint_prior_scale',\n",
    "        \"yearly_fourier_order\",\n",
    "        \"weekly_fourier_order\",\n",
    "        \"monthly_fourier_order\",\n",
    "\n",
    "    \"yearly_prior\",\n",
    "    \"weekly_prior\",\n",
    "    \"monthly_prior\",\n",
    "]\n",
    "\n",
    "\n",
    "have_kv_dict = {\n",
    "    \"yearly_fourier_order\": 1,\n",
    "    \"monthly_fourier_order\": 2,\n",
    "\n",
    "    \"yearly_prior\": 10,\n",
    "    \"monthly_prior\": 10,\n",
    "    \"yearly_on_season_prior\": 10, \n",
    "\n",
    "    \"weekly_fourier_order\": 5,\n",
    "    \"weekly_prior\": 18,\n",
    "    \n",
    "}\n",
    "\n",
    "other_cols = list(filter(lambda col: col not in [\"Date\"] + list(hierarchy.keys()) + list(hierarchy.values())[0] ,df.columns.tolist()))\n",
    "req_keys.extend(list(map(lambda col: \"{}_prior\".format(col) ,other_cols)))\n",
    "\n",
    "for col in other_cols:\n",
    "    have_kv_dict[\"{}_prior\".format(col)] = 10\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe\n",
    "\n",
    "\n",
    "tiny_space = {\n",
    "'n_changepoints': hp.choice(\"n_changepoints\", list(range(5, 45))),\n",
    " 'changepoint_range': hp.uniform(\"changepoint_range\", 0.5, 1.0),\n",
    " 'seasonality_prior_scale': hp.uniform(\"seasonality_prior_scale\", 5.0, 50.0),\n",
    " 'holidays_prior_scale': hp.uniform(\"holidays_prior_scale\", 5.0, 60.0),\n",
    " 'changepoint_prior_scale': hp.uniform(\"changepoint_prior_scale\", 0.0001, 0.1),\n",
    " 'yearly_fourier_order': hp.choice(\"yearly_fourier_order\", list(range(1, 70))),\n",
    " 'monthly_fourier_order': hp.choice(\"monthly_fourier_order\", list(range(1, 35))),\n",
    "    \n",
    "  \"yearly_prior\": hp.choice(\"yearly_prior\", np.logspace(1, 100, num = 100, base = 100 ** (1 / 100))),\n",
    "    \"monthly_prior\":hp.choice(\"monthly_prior\", np.logspace(1, 100, num = 100, base = 100 ** (1 / 100))),\n",
    "\n",
    "\"weekly_prior\":hp.choice(\"weekly_prior\", np.logspace(1, 100, num = 100, base = 100 ** (1 / 100))),\n",
    "'weekly_fourier_order': hp.choice(\"weekly_fourier_order\", list(range(1, 35))),\n",
    "    \n",
    "}\n",
    "\n",
    "for col in other_cols:\n",
    "    tiny_space[\"{}_prior\".format(col)] = hp.choice(\"{}_prior\".format(col), np.logspace(1, 100, num = 100, base = 100 ** (1 / 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "21d9c0b2-1a27-4a2c-94b0-1fe2bc0a77e2",
    "_uuid": "303dd769-c16f-4ad1-9059-6368ed80b184"
   },
   "outputs": [],
   "source": [
    "set_default_dict = dict(map(lambda k:(k, have_kv_dict[k]) if k in have_kv_dict else (k, prophet_defaults[k]), req_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "091db097-502d-4f64-bc9d-5eb509f6a0d8",
    "_uuid": "c36180a4-66b9-414b-aef3-98f200c656bc"
   },
   "outputs": [],
   "source": [
    "set_default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3de44cb9-13a1-4fcb-be8f-6948ccd9b8f4",
    "_uuid": "70198b22-6f35-4115-ab91-40eadc72b133"
   },
   "outputs": [],
   "source": [
    "#score = object_func(set_default_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d0d887d5-fbab-4354-9747-d4ee7f418daa",
    "_uuid": "9723f4a2-daf2-4962-a00c-ac51b8fb7a52"
   },
   "outputs": [],
   "source": [
    "#score\n",
    "#### with reg score 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9906bb54-07fe-4594-8168-a98d454a2187",
    "_uuid": "c51339a0-b7fc-40c0-9429-50b9f99c444f"
   },
   "outputs": [],
   "source": [
    "#score\n",
    "#### without reg score 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31f23280-fb66-4305-b9d8-217c6bb4415a",
    "_uuid": "9e3a1329-ab51-4098-8dcb-6b0f7d87098c"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import hyperopt\n",
    "import numpy as np\n",
    "\n",
    "PY2 = sys.version_info[0] == 2\n",
    "int_types = (int, long) if PY2 else (int,)\n",
    "\n",
    "\n",
    "def is_integer(obj):\n",
    "    return isinstance(obj, int_types + (np.integer,))\n",
    "\n",
    "\n",
    "def is_number(obj, check_complex=False):\n",
    "    types = ((float, complex, np.number) if check_complex else\n",
    "             (float, np.floating))\n",
    "    return is_integer(obj) or isinstance(obj, types)\n",
    "\n",
    "def get_vals(trial):\n",
    "    \"\"\"Determine hyperparameter values given a ``Trial`` object\"\"\"\n",
    "    # based on hyperopt/base.py:Trials:argmin\n",
    "    return dict((k, v[0]) for k, v in trial['misc']['vals'].items() if v)\n",
    "\n",
    "\n",
    "def wrap_cost(cost_fn, timeout=None, iters=1, verbose=0):\n",
    "    \"\"\"Wrap cost function to execute trials safely on a separate process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost_fn : callable\n",
    "        The cost function (aka. objective function) to wrap. It follows the\n",
    "        same specifications as normal Hyperopt cost functions.\n",
    "    timeout : int\n",
    "        Time to wait for process to complete, in seconds. If this time is\n",
    "        reached, the process is re-tried if there are remaining iterations,\n",
    "        otherwise marked as a failure. If ``None``, wait indefinitely.\n",
    "    iters : int\n",
    "        Number of times to allow the trial to timeout before marking it as\n",
    "        a failure due to timeout.\n",
    "    verbose : int\n",
    "        How verbose this function should be. 0 is not verbose, 1 is verbose.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    def objective(args):\n",
    "        case, val = args\n",
    "        return val**2 if case else val\n",
    "\n",
    "    space = [hp.choice('case', [False, True]), hp.uniform('val', -1, 1)]\n",
    "    safe_objective = wrap_cost(objective, timeout=2, iters=2, verbose=1)\n",
    "    best = hyperopt.fmin(safe_objective, space, max_evals=100)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Based on code from https://github.com/hyperopt/hyperopt-sklearn\n",
    "    \"\"\"\n",
    "    def _cost_fn(*args, **kwargs):\n",
    "        _conn = kwargs.pop('_conn')\n",
    "        try:\n",
    "            t_start = time.time()\n",
    "            rval = cost_fn(*args, **kwargs)\n",
    "            t_done = time.time()\n",
    "\n",
    "            if not isinstance(rval, dict):\n",
    "                rval = dict(loss=rval)\n",
    "            assert 'loss' in rval, \"Returned dictionary must include loss\"\n",
    "            loss = rval['loss']\n",
    "            assert is_number(loss), \"Returned loss must be a number type\"\n",
    "            rval.setdefault('status', hyperopt.STATUS_OK if np.isfinite(loss)\n",
    "                            else hyperopt.STATUS_FAIL)\n",
    "            rval.setdefault('duration', t_done - t_start)\n",
    "            rtype = 'return'\n",
    "\n",
    "        except Exception as exc:\n",
    "            rval = exc\n",
    "            rtype = 'raise'\n",
    "\n",
    "        # -- return the result to calling process\n",
    "        _conn.send((rtype, rval))\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        for k in range(iters):\n",
    "            conn1, conn2 = Pipe()\n",
    "            kwargs['_conn'] = conn2\n",
    "            th = Process(target=_cost_fn, args=args, kwargs=kwargs)\n",
    "            th.start()\n",
    "            if conn1.poll(timeout):\n",
    "                fn_rval = conn1.recv()\n",
    "                th.join()\n",
    "            else:\n",
    "                if verbose >= 1:\n",
    "                    print(\"TRIAL TIMED OUT (%d/%d)\" % (k+1, iters))\n",
    "                th.terminate()\n",
    "                th.join()\n",
    "                continue\n",
    "\n",
    "            assert fn_rval[0] in ('raise', 'return')\n",
    "            if fn_rval[0] == 'raise':\n",
    "                raise fn_rval[1]\n",
    "            else:\n",
    "                return fn_rval[1]\n",
    "\n",
    "        return {'status': hyperopt.STATUS_FAIL,\n",
    "                'failure': 'timeout'}\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "#### try two times\n",
    "safe_objective = wrap_cost(object_func, timeout=500, iters=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f086b46-5385-48c8-a712-8032fd63b232",
    "_uuid": "6ed7a4ca-a2a0-4a40-b226-4b4e3353ebd3"
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "trials = Trials()\n",
    "\n",
    "#### with calculate\n",
    "def run_trials():\n",
    "    import pickle\n",
    "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "    trials_path = \"/kaggle/working/trials_mean_1_peak_100_prior.pkl\"\n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        #trials = pickle.load(open(\"my_model.hyperopt\", \"rb\"))x\n",
    "        trials = pickle.load(open(trials_path, \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    #best = fmin(fn=mymodel, space=model_space, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "    try:\n",
    "        best_without_init = fmin(safe_objective, tiny_space, algo = tpe.suggest, max_evals = max_trials,  points_to_evaluate=[set_default_dict], \n",
    "                         trials = trials)\n",
    "    except:\n",
    "        print(\"Time Out !!!!\")\n",
    "        return\n",
    "    print(\"Best:\", best_without_init)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(trials_path, \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "        \n",
    "    print(\"length : {}\".format(len(trials)) + \"!\" * 100)\n",
    "\n",
    "# loop indefinitely and stop whenever you like\n",
    "times = 0\n",
    "while True:\n",
    "    run_trials()\n",
    "    '''\n",
    "    times += 1 \n",
    "    if times >= 3:\n",
    "        break\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
